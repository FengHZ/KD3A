---
DataConfig:
  dataset: "AmazonReview"

ModelConfig:
  backbone: "AmazonMLP"
  pretrained: False

TrainingConfig:
  batch_size: 50
  # The total data numbers we use in each epoch
  epoch_samples: 2000
  total_epochs: 100
  # We decay learning rate from begin value to end value with cosine annealing schedule
  learning_rate_begin: 0.05
  learning_rate_end: 0.001

# The configuration for our decentralized unsupervised multi-source domain adaptation
UMDAConfig:
  # As stated in paper, we gradually increase confidence_gate from low to high
  confidence_gate_begin: 0.9
  confidence_gate_end: 0.95
  # Controlling whether to use the batchnorm_mmd
  batchnorm_mmd: True
  # the communication rounds in decentralized training, can be set into [0.2, 0.5 ,1, N]
  communication_rounds: 1
  # the malicious domain with poisoning attack
  malicious:
    attack_domain: "dvd"
    # attack_level is a float number stands for the mislabeled ratio m%
    attack_level: 0








